ğŸ§  Intelligent Web Chat Scraper
An AI-powered web scraping and chat interface that extracts, analyzes, and responds to user queries using Hugging Face Transformers. It supports website scraping with Selenium fallback, allowing it to handle both static and dynamic pages â€” even those that block normal scraping.
ğŸš€ Features

ğŸ—£ï¸ Conversational Chat Interface â€” Chat with the system like a human.
ğŸ§© AI-Powered Inference â€” Uses Hugging Face Transformers for intelligent text understanding and generation.
ğŸŒ Smart Web Scraping â€” Automatically scrapes website content using BeautifulSoup and Selenium fallback for dynamic pages.
ğŸ“‚ Multi-Website Support â€” Works across many websites and formats.
ğŸ’¾ Chunk Storage â€” Saves processed website data locally in chunks.json for faster repeated queries.
ğŸ§± Resilient Architecture â€” Automatically retries failed scrapes, skips broken URLs, and handles errors gracefully.
âš™ï¸ Customizable Parameters â€” Control model, temperature, and max token limits directly from UI (if using Gradio).

ğŸ§° Tech Stack
LayerTechnologyFrontend / InterfaceStreamlit / GradioBackendPython, Flask (optional)AI / NLPHugging Face Transformers, SentencePieceScrapingBeautifulSoup, Selenium (fallback)Data StorageJSON (chunks.json), CSVDeploymentHugging Face Spaces / GitHub Pages
âš™ï¸ How It Works

User enters a query or URL.
The system scrapes the page using requests + BeautifulSoup.
If content is dynamically loaded, Selenium is used as fallback.
Scraped text is split into chunks and saved in chunks.json.
AI model processes these chunks via Hugging Face Transformers to generate intelligent responses.
Processed output is displayed in a clean chat interface.

ğŸ“ Project Structure
ğŸ“¦ API-GUARDIAN/
â”œâ”€â”€ __pycache__/          # Python cache files
â”œâ”€â”€ embeddings/           # Embedded vector representations
â”œâ”€â”€ .gitignore           # Git ignore configuration
â”œâ”€â”€ output/              # Generated output files
â”œâ”€â”€ venv/                # Virtual environment
â”œâ”€â”€ .env                 # Environment variables
â”œâ”€â”€ app.py               # Main application entry point
â”œâ”€â”€ config.py            # Configuration settings
â”œâ”€â”€ rag_pipeline.py      # RAG (Retrieval-Augmented Generation) pipeline
â”œâ”€â”€ requirements.txt     # All dependencies
â””â”€â”€ scraper.py           # Web scraping logic (BeautifulSoup + Selenium fallback)
ğŸ”§ Setup Instructions
1ï¸âƒ£ Clone the Repository
bashgit clone https://github.com/<your-username>/API-GUARDIAN.git
cd API-GUARDIAN
2ï¸âƒ£ Set Up Virtual Environment
bashpython -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate
3ï¸âƒ£ Configure Environment Variables
Create a .env file in the root directory with your configuration:
envHF_TOKEN=your_huggingface_token_here
# Add other API keys and configurations as needed
4ï¸âƒ£ Install Dependencies
bashpip install -r requirements.txt
5ï¸âƒ£ Run the App
If using Gradio:
bashpython app.py
If using Streamlit:
bashstreamlit run app.py
ğŸ”‘ Hugging Face Deployment
If deployed on Hugging Face Spaces:

Add your HF_TOKEN as a repository secret under Settings â†’ Repository Secrets â†’ New secret
The token will be automatically loaded from environment variables

ğŸ“¸ Snapshots
Show Image
Show Image
Show Image
Show Image
Show Image
ğŸ§  Example Use
Input:
"Scrape the website https://example.com and summarize key topics."
Output:
"The website discusses machine learning frameworks, datasets, and AI benchmarks, focusing on model training efficiency."
ğŸ§© Future Improvements

Add offline mode using Ollama for local AI inference.
Integrate vector database (FAISS / ChromaDB) for advanced retrieval.
Improve chunk storage efficiency and indexing.
Add multi-language support and voice interface.
Enhance RAG pipeline with better context retrieval.

ğŸ‘¨â€ğŸ’» Authors
Developed by: Basavaraj M N
Institution: KLE Institute of Technology, Hubli
Role: Student & Developer
ğŸªª License
This project is licensed under the MIT License â€” feel free to use, modify, and share.

ğŸ“ Notes

The embeddings/ directory stores vector representations for efficient similarity search.
The output/ directory contains generated files and logs.
config.py centralizes all configuration settings for easy management.
rag_pipeline.py implements the Retrieval-Augmented Generation system for enhanced AI responses.
